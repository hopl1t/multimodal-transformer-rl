{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59c31c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-16 16:18:43.421605: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-16 16:18:43.504644: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-16 16:18:43.934211: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-16 16:18:43.934247: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-16 16:18:43.934251: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.nn import Identity\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "import dill\n",
    "from textattack.models.wrappers.huggingface_model_wrapper import HuggingFaceModelWrapper\n",
    "from transformers import PreTrainedModel\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from textattack import Attacker, AttackArgs\n",
    "from textattack import Attacker\n",
    "from textattack.attack_recipes import BAEGarg2019\n",
    "from textattack.datasets import HuggingFaceDataset\n",
    "from textattack.attack_results import AttackResult\n",
    "from textattack.metrics.attack_metrics import (\n",
    "    AttackQueries,\n",
    "    AttackSuccessRate,\n",
    "    WordsPerturbed,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d7d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerVIB(nn.Module):\n",
    "    \"\"\"\n",
    "    Classifier with stochastic layer and KL regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, output_size, device):\n",
    "        super(TransformerVIB, self).__init__()\n",
    "        self.device = device\n",
    "        self.description = 'Vanilla IB VAE as per the paper'\n",
    "        self.hidden_size = hidden_size\n",
    "        self.k = hidden_size // 2\n",
    "        self.output_size = output_size\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Linear(self.k, output_size)\n",
    "        # These are cheats to make 'drill' save everythung we need in one pickle\n",
    "        self.softplus = F.softplus\n",
    "        self.normal = torch.normal\n",
    "        self.Normal = Normal\n",
    "\n",
    "        # Xavier initialization\n",
    "        for _, module in self._modules.items():\n",
    "            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
    "                        nn.init.xavier_uniform_(module.weight, gain=nn.init.calculate_gain('relu'))\n",
    "                        module.bias.data.zero_()\n",
    "                        continue\n",
    "            for layer in module:\n",
    "                if isinstance(layer, nn.Linear) or isinstance(layer, nn.Conv2d):\n",
    "                            nn.init.xavier_uniform_(layer.weight, gain=nn.init.calculate_gain('relu'))\n",
    "                            layer.bias.data.zero_()\n",
    "        \n",
    "\n",
    "    def reparametrize(self, mu, std, device):\n",
    "        \"\"\"\n",
    "        Performs reparameterization trick z = mu + epsilon * std\n",
    "        Where epsilon~N(0,1)\n",
    "        \"\"\"\n",
    "        mu = mu.expand(1, *mu.size())\n",
    "        std = std.expand(1, *std.size())\n",
    "        eps = self.normal(0, 1, size=std.size()).to(device)\n",
    "        return mu + eps * std        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        z_params = self.encoder(x)\n",
    "        mu = z_params[:, :self.k]\n",
    "#         std = torch.nn.functional.softplus(z_params[:, self.k:] - 1, beta=1)        \n",
    "        std = self.softplus(z_params[:, self.k:] - 1, beta=1)        \n",
    "        if self.training:\n",
    "            z = self.reparametrize(mu, std, self.device)\n",
    "        else:\n",
    "            z = mu.clone().unsqueeze(0)\n",
    "        n = self.Normal(mu, std)\n",
    "        log_probs = n.log_prob(z.squeeze(0))  # These may be positive as this is a PDF\n",
    "        \n",
    "        logits = self.classifier(z)\n",
    "        return (mu, std), log_probs, logits\n",
    "   \n",
    "\n",
    "class TransformerHybridModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Head is a pretrained model, classifier is VIB\n",
    "    fc_name should be 'fc2' for inception-v3 (imagenet) and mnist-cnn, '_fc' for efficient-net (CIFAR)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model, vib_model, device, fc_name, return_only_logits=False):\n",
    "        super(TransformerHybridModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.base_model = base_model\n",
    "        setattr(self.base_model, fc_name, torch.nn.Identity())\n",
    "        self.vib_model = vib_model\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        self.freeze_base()\n",
    "        self.return_only_logits = return_only_logits\n",
    "\n",
    "    def set_return_only_logits(self, bool_value):\n",
    "        self.return_only_logits = bool_value\n",
    "    \n",
    "    def freeze_base(self):\n",
    "        # Freeze the weights of the inception_model\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_base(self):\n",
    "        # Freeze the weights of the inception_model\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        encoded = self.base_model(kwargs['input_ids']).logits # This is not really logits, only called that way cause we've changed the final layer to identity\n",
    "        (mu, std), log_probs, logits = self.vib_model(encoded)\n",
    "        if self.return_only_logits:\n",
    "            return logits.squeeze(0)\n",
    "        else:\n",
    "            return ((mu, std), log_probs, logits)\n",
    "\n",
    "\n",
    "class TransformerAdaptor(transformers.PreTrainedModel):\n",
    "    \"\"\"\n",
    "    Adapts between a TransformerHybridModel to a HuggingFaceModelWrapper\n",
    "    \"\"\"\n",
    "    def __init__(self, hybrid_model):\n",
    "        super(TransformerAdaptor, self).__init__(hybrid_model.base_model.config)\n",
    "        self.hybrid_model = hybrid_model\n",
    "        self.SequenceClassifierOutput = SequenceClassifierOutput  # Cheat to overload drill pickle\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        if self.hybrid_model.return_only_logits:\n",
    "            logits = self.hybrid_model(**kwargs)\n",
    "        else:\n",
    "            ((_, _), _, logits) = self.hybrid_model(**kwargs)\n",
    "        return self.SequenceClassifierOutput(logits=logits[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a752ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model\n",
    "model = BertForSequenceClassification.from_pretrained('textattack/bert-base-uncased-yelp-polarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "69bfefae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/D/models/pretrained/pretrained_bert_yelp.pkl', 'wb') as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb698b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "683e00c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vib_classifier = TransformerVIB(768, 2, device)\n",
    "hybrid_model = TransformerHybridModel(model, vib_classifier, device, fc_name='classifier')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# wrapper_model = HuggingFaceModelWrapper(model, tokenizer)\n",
    "\n",
    "adaptor = TransformerAdaptor(hybrid_model)\n",
    "\n",
    "wrapper = HuggingFaceModelWrapper(adaptor, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "71cc04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/D/models/bert/bert_yelp_vib.pkl', 'wb') as f:\n",
    "    dill.dump(hybrid_model, f)\n",
    "    \n",
    "with open('/D/models/bert/bert_yelp_vib_wrapper.pkl', 'wb') as f:\n",
    "    dill.dump(wrapper, f)\n",
    "    \n",
    "with open('/D/models/bert/bert_yelp_vib_adaptor.pkl', 'wb') as f:\n",
    "    dill.dump(adaptor, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70babf4",
   "metadata": {},
   "source": [
    "### Yelp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58ab91f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset yelp_polarity (/home/nir/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/14f90415c754f47cf9087eadac25823a395fef4400c7903c5897f55cfaaa6f61)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d2eaa9f4014ef8a5389e2ca65f4633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Yelp Polarity dataset\n",
    "dataset = load_dataset('yelp_polarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab387c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/nir/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/14f90415c754f47cf9087eadac25823a395fef4400c7903c5897f55cfaaa6f61/cache-f43dc4f27a40156d.arrow\n",
      "Loading cached processed dataset at /home/nir/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/14f90415c754f47cf9087eadac25823a395fef4400c7903c5897f55cfaaa6f61/cache-85110606db85e919.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "def encode(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "dataset = dataset.map(encode, batched=True)\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "dataset.save_to_disk('/D/datasets/yelp/')\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70b6af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = load_from_disk('/D/datasets/yelp/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29791ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogitsDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        return x, y\n",
    "\n",
    "def get_transformer_logits_dataloader(model, original_loader, device, batch_size=8):\n",
    "    logits_data_list = []\n",
    "    logits_labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for data_dict in tqdm(original_loader):\n",
    "            x = data_dict['input_ids']\n",
    "            y = data_dict['label']\n",
    "            output = model(x.to(device))\n",
    "            logits = output.logits\n",
    "            logits_data_list.append(logits.to(torch.device('cpu')))\n",
    "            logits_labels_list.append(y.to(torch.device('cpu')))\n",
    "\n",
    "    logits_data_set = LogitsDataset(torch.concat(logits_data_list), torch.concat(logits_labels_list))\n",
    "    logits_dataloader = DataLoader(logits_data_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return logits_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "192ac17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=128)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eeea3d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4375/4375 [4:04:53<00:00,  3.36s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 297/297 [16:40<00:00,  3.37s/it]\n"
     ]
    }
   ],
   "source": [
    "model.classifier = torch.nn.Identity()\n",
    "DEVICE = torch.device('cuda:0')\n",
    "model.to(DEVICE)\n",
    "logits_trian_dataloader = get_transformer_logits_dataloader(model, train_dataloader, batch_size=64, device=DEVICE)\n",
    "logits_test_dataloader = get_transformer_logits_dataloader(model, test_dataloader, batch_size=64, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ce86faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGITS_TRAIN_DATALOADER_PATH = '/D/datasets/yelp/logits_dataloaders/logits_train_dataloader.pkl'\n",
    "LOGITS_TEST_DATALOADER_PATH = '/D/datasets/yelp/logits_dataloaders/logits_test_dataloader.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "844ac533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataloaders!\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "import pickle5\n",
    "with open(LOGITS_TRAIN_DATALOADER_PATH, 'wb') as f:\n",
    "    pickle5.dump(logits_trian_dataloader, f)\n",
    "with open(LOGITS_TEST_DATALOADER_PATH, 'wb') as f:\n",
    "    pickle5.dump(logits_test_dataloader, f)\n",
    "print('Saved dataloaders!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e2197ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5\n",
    "with open(LOGITS_TRAIN_DATALOADER_PATH, 'rb') as f:\n",
    "    logits_trian_dataloader = pickle5.load(f)\n",
    "with open(LOGITS_TEST_DATALOADER_PATH, 'rb') as f:\n",
    "    logits_test_dataloader = pickle5.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad29f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "vib_classifier = TransformerVIB(768, 2, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50bda224",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('textattack/bert-base-uncased-yelp-polarity')\n",
    "DEVICE = torch.device('cuda:0')\n",
    "_ = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe6d4355",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0957a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data_loader, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_incorrect = 0\n",
    "    with torch.no_grad():\n",
    "        for data_dict in tqdm(test_data_loader):\n",
    "            x = data_dict['input_ids']\n",
    "            y = data_dict['label']\n",
    "            output = model(x.to(device))\n",
    "            logits = output['logits']\n",
    "            predictions = torch.argmax(torch.softmax(logits, dim=-1), dim=1)#.to(torch.device('cpu'))\n",
    "            correct_classifications = sum(predictions == y.to(device))\n",
    "            incorrect_classifications = len(x) - correct_classifications\n",
    "            total_correct += correct_classifications\n",
    "            total_incorrect += incorrect_classifications\n",
    "    model.train()\n",
    "    print(f\"acc: {print(total_correct / (total_correct + total_incorrect))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "364dc6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1188/1188 [15:03<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8969, device='cuda:0')\n",
      "acc: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_model(model, test_dataloader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46966ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/D/models/pretrained/pretrained_bert_yelp.pkl', 'rb') as f:\n",
    "    m = pickle.load(f)\n",
    "# torch.load('/D/models/pretrained/pretrained_bert_yelp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59abbdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset yelp_polarity (/home/nir/.cache/huggingface/datasets/yelp_polarity/plain_text/1.0.0/14f90415c754f47cf9087eadac25823a395fef4400c7903c5897f55cfaaa6f61)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5aacfcfafaf4cae9a7d528d394f56fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94myelp_polarity\u001b[0m, split \u001b[94mtest\u001b[0m.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "textattack: Unknown if model of class <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
      "[Succeeded / Failed / Skipped / Total] 148 / 98 / 4 / 250: 100%|████████████████████████████████████████████████████████████| 250/250 [44:18<00:00, 10.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ec75ade2b64961bb9ac8450379b42c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10eb7843494645979274d9b3deb02942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362981d3a0b14723adde6be761da48d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021a2dd11f1d411b97fb02117c5f4a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca843f7456b4dcdbc416052ca9c492f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32148 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (32148 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('textattack/bert-base-uncased-yelp-polarity')\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "dataset = HuggingFaceDataset(\"yelp_polarity\", None, \"test\")\n",
    "\n",
    "model_wrapper = HuggingFaceModelWrapper(model, tokenizer)\n",
    "\n",
    "attack = BAEGarg2019.build(model_wrapper)\n",
    "attack_args = AttackArgs(\n",
    "    num_examples=250,\n",
    "    disable_stdout=True,\n",
    "    silent=True,\n",
    "    enable_advance_metrics=True\n",
    ")\n",
    "\n",
    "attacker = Attacker(attack, dataset, attack_args)\n",
    "results_iterable = attacker.attack_dataset()\n",
    "attack_log_manager = attacker.attack_log_manager\n",
    "attack_log_manager.log_summary()\n",
    "\n",
    "attack_success_stats = AttackSuccessRate().calculate(attack_log_manager.results)\n",
    "words_perturbed_stats = WordsPerturbed().calculate(attack_log_manager.results)\n",
    "attack_query_stats = AttackQueries().calculate(attack_log_manager.results)\n",
    "acc_under_attack = str(attack_success_stats[\"attack_accuracy_perc\"])\n",
    "avg_pertrubed_words_prct = str(words_perturbed_stats[\"avg_word_perturbed_perc\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8c96eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table_rows = [\n",
    "    [\n",
    "        \"Number of successful attacks:\",\n",
    "        attack_success_stats[\"successful_attacks\"],\n",
    "    ],\n",
    "    [\"Number of failed attacks:\", attack_success_stats[\"failed_attacks\"]],\n",
    "    [\"Number of skipped attacks:\", attack_success_stats[\"skipped_attacks\"]],\n",
    "    [\n",
    "        \"Original accuracy:\",\n",
    "        str(attack_success_stats[\"original_accuracy\"]) + \"%\",\n",
    "    ],\n",
    "    [\n",
    "        \"Accuracy under attack:\",\n",
    "        str(attack_success_stats[\"attack_accuracy_perc\"]) + \"%\",\n",
    "    ],\n",
    "    [\n",
    "        \"Attack success rate:\",\n",
    "        str(attack_success_stats[\"attack_success_rate\"]) + \"%\",\n",
    "    ],\n",
    "    [\n",
    "        \"Average perturbed word %:\",\n",
    "        str(words_perturbed_stats[\"avg_word_perturbed_perc\"]) + \"%\",\n",
    "    ],\n",
    "    [\n",
    "        \"Average num. words per input:\",\n",
    "        words_perturbed_stats[\"avg_word_perturbed\"],\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b42dd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_success_stats[\"original_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dcfa42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Number of successful attacks:', 148]\n",
      "['Number of failed attacks:', 98]\n",
      "['Number of skipped attacks:', 4]\n",
      "['Original accuracy:', '98.4%']\n",
      "['Accuracy under attack:', '39.2%']\n",
      "['Attack success rate:', '60.16%']\n",
      "['Average perturbed word %:', '7.01%']\n",
      "['Average num. words per input:', 136.34]\n"
     ]
    }
   ],
   "source": [
    "for row in summary_table_rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56e0da10",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_log_manager.log_summary_rows(\n",
    "    summary_table_rows, \"Attack Results\", \"attack_results_summary\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
